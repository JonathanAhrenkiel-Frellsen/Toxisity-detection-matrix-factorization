{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import rand as sprand\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make up some random explicit feedback ratings\n",
    "# and convert to a numpy array\n",
    "n_users = 1000\n",
    "n_items = 1000\n",
    "ratings = sprand(n_users, n_items, density=0.01, format=\"csr\")\n",
    "ratings.data = np.random.randint(0, 2, size=ratings.nnz).astype(np.float64)\n",
    "ratings = ratings.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0 ...  0  0  0]\n",
      " [ 1  0  0 ... -1  1  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0 -1 ...  0  1  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "responses = [-1, 0, 1]\n",
    "p = np.array([1, 5, 1])\n",
    "m = 1000\n",
    "n = 1000\n",
    "\n",
    "# A binary response matrix.\n",
    "ratings = np.random.choice(responses, size=m*n, p=p / p.sum()).reshape((m, n))\n",
    "print(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorization(torch.nn.Module):\n",
    "    def __init__(self, n_users, n_items, n_factors=20):\n",
    "        super().__init__()\n",
    "        self.user_factors = torch.nn.Embedding(n_users, n_factors, sparse=True)\n",
    "        self.item_factors = torch.nn.Embedding(n_items, n_factors, sparse=True)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        return (self.user_factors(user) * self.item_factors(item)).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MatrixFactorization(n_users, n_items, n_factors=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-6)  # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], grad_fn=<SignBackward>)\n",
      "tensor([-1.])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-e17506afec23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrating\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrating\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# Backpropagate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    960\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    961\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m--> 962\u001b[1;33m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[0;32m    963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2466\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2467\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2468\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1603\u001b[0m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'log_softmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1604\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1605\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1606\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "# Sort our data\n",
    "rows, cols = ratings.nonzero()\n",
    "p = np.random.permutation(len(rows))\n",
    "rows, cols = rows[p][:10], cols[p][:10]\n",
    "\n",
    "for row, col in zip(*(rows, cols)):\n",
    "    # Set gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Turn data into tensors\n",
    "    rating = torch.FloatTensor([ratings[row, col]])\n",
    "    row = torch.LongTensor([row])\n",
    "    col = torch.LongTensor([col])\n",
    "\n",
    "    # Predict and calculate loss\n",
    "    prediction = model(row, col)\n",
    "    print(torch.sign(prediction))\n",
    "    print(rating)\n",
    "    loss = loss_func(torch.sign(prediction), rating)\n",
    "\n",
    "    # Backpropagate\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MatrixFactorization(\n",
       "  (user_factors): Embedding(1000, 20, sparse=True)\n",
       "  (item_factors): Embedding(1000, 20, sparse=True)\n",
       ")"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasedMatrixFactorization(torch.nn.Module):\n",
    "    def __init__(self, n_users, n_items, n_factors=20):\n",
    "        super().__init__()\n",
    "        self.user_factors = torch.nn.Embedding(n_users, n_factors, sparse=True)\n",
    "        self.item_factors = torch.nn.Embedding(n_items, n_factors, sparse=True)\n",
    "        self.user_biases = torch.nn.Embedding(n_users, 1, sparse=True)\n",
    "        self.item_biases = torch.nn.Embedding(n_items, 1, sparse=True)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        pred = self.user_biases(user) + self.item_biases(item)\n",
    "        pred += (\n",
    "            (self.user_factors(user) * self.item_factors(item))\n",
    "            .sum(dim=1, keepdim=True)\n",
    "        )\n",
    "        return pred.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_loss_func = torch.optim.SGD(model.parameters(), lr=1e-6, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "adagrad_loss = torch.optim.Adagrad(model.parameters(), lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adagrad (\n",
       "Parameter Group 0\n",
       "    eps: 1e-10\n",
       "    initial_accumulator_value: 0\n",
       "    lr: 1e-06\n",
       "    lr_decay: 0\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adagrad_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "tensor([-1.])\n",
      "tensor([0])\n",
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "rows, cols = ratings.nonzero()\n",
    "i = 0\n",
    "for row, col in zip(*(rows, cols)):\n",
    "    if (i < 1):\n",
    "        print(row)\n",
    "        print(col)\n",
    "        \n",
    "        # Turn data into tensors\n",
    "        rating = torch.FloatTensor([ratings[row, col]])\n",
    "        row = torch.LongTensor([row])\n",
    "        col = torch.LongTensor([col])\n",
    "        \n",
    "        \n",
    "\n",
    "        # Predict and calculate loss\n",
    "        prediction = model(row, col)\n",
    "        \n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.])\n",
      "tensor([3.7548], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "row = 0\n",
    "col = 15\n",
    "\n",
    "\n",
    "row = torch.LongTensor([row])\n",
    "col = torch.LongTensor([col])\n",
    "rating = torch.FloatTensor([ratings[row, col]])\n",
    "\n",
    "prediction = model(row, col)\n",
    "\n",
    "print(rating)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0,   0,   0, ..., 999, 999, 999], dtype=int64),\n",
       " array([  0,  11,  15, ..., 990, 995, 996], dtype=int64))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings[999,996]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9043,  0.2982,  0.4213,  0.5203,  2.3286],\n",
       "        [-0.1868, -0.0185,  0.5554, -0.4583,  0.9682],\n",
       "        [-1.3282,  0.4659, -0.8215, -0.2858,  0.4146]], requires_grad=True)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 4])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1274, -2.1894,  1.8774,  0.3813,  0.7972],\n",
       "        [-0.3498,  0.0927, -1.1307,  0.7418,  0.4693],\n",
       "        [-0.7773,  0.0807,  0.4787,  1.3048,  0.0729]], requires_grad=True)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3, 5, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3202, 0.2247, 0.1018, 0.2388, 0.1145],\n",
       "        [0.5049, 0.0781, 0.0201, 0.2026, 0.1943],\n",
       "        [0.0821, 0.1649, 0.0278, 0.4249, 0.3004]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3, 5).softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "output = torch.randn(10, 120).float()\n",
    "target = torch.FloatTensor(10).uniform_(0, 120).long()\n",
    "\n",
    "loss = criterion(output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7036, -0.4430, -1.2327,  ...,  1.3873, -0.0058,  0.7460],\n",
       "        [ 1.3635,  0.8821, -0.0163,  ...,  0.8061, -0.1501,  0.7539],\n",
       "        [-1.7296, -0.2423,  0.0781,  ...,  0.1397, -0.6892,  0.6489],\n",
       "        ...,\n",
       "        [ 1.2435, -0.8681,  0.9451,  ...,  0.3858, -0.2553,  1.9310],\n",
       "        [-0.8657, -1.4592, -1.9062,  ...,  0.5828,  1.4260, -1.1177],\n",
       "        [-1.1548, -0.0904, -1.6029,  ..., -1.9163,  0.0331,  1.1120]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 78,  48,  46,  26,  65,   1, 106,  59,  79,  96])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorization:\n",
    "  def __init__(self, R, k, lr=.0003, l2=.04, seed=777):\n",
    "    self.R = tf.convert_to_tensor(R, dtype=tf.float32)\n",
    "    self.mask = tf.not_equal(self.R, 0)\n",
    "    self.m, self.n = R.shape\n",
    "    self.k = k\n",
    "    self.lr = lr\n",
    "    self.l2 = l2\n",
    "    self.tol = .001\n",
    "    # Initialize trainable weights.\n",
    "    self.weight_init = tf.random_normal_initializer(seed=seed)\n",
    "    self.P = tf.Variable(self.weight_init((self.m, self.k)))\n",
    "    self.Q = tf.Variable(self.weight_init((self.n, self.k)))\n",
    "\n",
    "  def loss(self):\n",
    "    raise NotImplementedError\n",
    "\n",
    "  def grad_update(self):\n",
    "    with tf.GradientTape() as t:\n",
    "      t.watch([self.P, self.Q])\n",
    "      self.current_loss = self.loss()\n",
    "    gP, gQ = t.gradient(self.current_loss, [self.P, self.Q])\n",
    "    self.P.assign_sub(self.lr * gP)\n",
    "    self.Q.assign_sub(self.lr * gQ)\n",
    "\n",
    "  def train(self, n_epoch=5000):\n",
    "    for epoch in range(n_epoch):\n",
    "      self.grad_update()\n",
    "      if self.current_loss < self.tol:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  1  0  0 -1  1  0  1  0]\n",
      " [ 0  0  1  0 -1  1  0  0  0  0]\n",
      " [-1  0  0  0  1 -1  0  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0  0 -1  1]\n",
      " [ 0  1  0  0  0 -1  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# Make missing more prevail.\n",
    "responses = [-1, 0, 1]\n",
    "p = np.array([1, 5, 1])\n",
    "m = 5\n",
    "n = 10\n",
    "\n",
    "# A binary response matrix.\n",
    "b_ratings = np.random.choice(responses, size=m*n, p=p / p.sum()).reshape((m, n))\n",
    "print(b_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryMF(MatrixFactorization):\n",
    "  def train(self, n_epoch=5000):\n",
    "    # Cast 1/-1 as binary encoding of 0/1.\n",
    "    self.labels = tf.cast(tf.not_equal(tf.boolean_mask(self.R, self.mask), -1), dtype=tf.float32)\n",
    "    for epoch in range(n_epoch):\n",
    "      self.grad_update()\n",
    "\n",
    "  # The implementation is far from optimized since we don't need the product of entire P'Q.\n",
    "  # We only need scores for non-missing entries.\n",
    "  # The code is hence for educational purpose only.\n",
    "  def loss(self):\n",
    "    \"\"\"Cross entropy loss.\"\"\"\n",
    "    logits = tf.boolean_mask(tf.matmul(self.P, self.Q, transpose_b=True), self.mask)\n",
    "    logloss = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.labels, logits=logits)\n",
    "    mlogloss = tf.reduce_mean(logloss)\n",
    "    l2_norm = tf.reduce_sum(self.P**2) + tf.reduce_sum(self.Q**2)\n",
    "    return mlogloss + self.l2 * l2_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.   0.99 0.   0.   0.03 0.98 0.   0.99 0.  ]\n",
      " [0.   0.   0.99 0.   0.01 0.99 0.   0.   0.   0.  ]\n",
      " [0.02 0.   0.   0.   0.99 0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.02 0.   0.   0.   0.   0.01 0.98]\n",
      " [0.   0.96 0.   0.   0.   0.01 0.   0.   0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "# We increase the learning a bit since logloss has a very different scale than squared error.\n",
    "# For the same reason we decrease the L2 coefficient.\n",
    "bmf_model = BinaryMF(b_ratings, k=3, lr=.03, l2=.0001)\n",
    "bmf_model.train()\n",
    "\n",
    "b_predictions = tf.sigmoid(tf.matmul(bmf_model.P, bmf_model.Q, transpose_b=True)).numpy()\n",
    "\n",
    "b_mask = np.zeros_like(b_ratings)\n",
    "b_mask[b_ratings.nonzero()] = 1\n",
    "\n",
    "print(np.round(b_predictions * b_mask, 2)) # Check prediction on training entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.34 0.22 0.99 0.92 0.24 0.03 0.98 0.48 0.99 0.08]\n",
      " [0.97 0.04 0.99 0.92 0.01 0.99 0.93 0.48 0.97 0.07]\n",
      " [0.02 0.96 0.04 0.1  0.99 0.   0.2  0.52 0.08 0.91]\n",
      " [0.09 0.97 0.02 0.02 0.97 0.18 0.04 0.54 0.01 0.98]\n",
      " [0.04 0.96 0.05 0.08 0.98 0.01 0.16 0.52 0.05 0.93]]\n"
     ]
    }
   ],
   "source": [
    "print(np.round(b_predictions, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
